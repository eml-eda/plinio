{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f978ca1",
   "metadata": {
    "id": "0f978ca1"
   },
   "source": [
    "# Quantization-Aware Training as a Corner Case of Mixed-Precision Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabf2b8-fef7-4474-9098-6eb1d53a3c4d",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3898ee-c56c-4138-9d0f-b1f843ae84fd",
   "metadata": {},
   "source": [
    "Install requirements for this and other examples as follows:\n",
    "```bash\n",
    "pip install -r <plinio_folder>/examples/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac54ad-060e-4546-9dd9-1df9c5f5c181",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2bcfe",
   "metadata": {
    "id": "59c2bcfe"
   },
   "source": [
    "PLiNIO can be used to perform **Quantization-Aware Training (QAT)**, producing \"full integer\" models compatible with edge devices without a Floating Point Unit (FPU).  PLiNIO's QAT function is embedded in the `MPS()` class, which performs *Mixed-Precision Search*. Namely, it can apply QAT at *multiple bit-widths* simultaneously, and use a SuperNet-like method to select the best precision assignment for the weights and activations of different portions of a DNN (different layers, or even different channels of the same layer), balancing accuracy and inference cost.\n",
    "\n",
    "To implement a \"standard\" single-precision QAT, we can simply reduce it to a **\"corner case\" of MPS, with a single precision** (8-bit in this example).\n",
    "\n",
    "If you're interested in the details on the MPS algorithm present in PLiNIO, check-out these  papers: [link1](https://arxiv.org/abs/2407.01054) [link2](https://arxiv.org/abs/2004.05795) and the library documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90",
   "metadata": {
    "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3",
   "metadata": {
    "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3"
   },
   "source": [
    "We start by importing required libraries. The details of PLiNIO imports are the following:\n",
    "- The `MPS` class implements the MPS/QAT optimization method.\n",
    "- The `get_default_qinfo` function returns a dictionary containing default quantization settings.\n",
    "- The `PACTActSigned` class implements [PACT](https://arxiv.org/abs/1805.06085) activations quantization for signed data and is used to customize the quantization behaviour for our network's input (see below).\n",
    "- Imports from the `backends` sub-package are required to export an ONNX file compatible with the [MATCH](https://arxiv.org/abs/2410.08855) AI Compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
   "metadata": {
    "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# for loss/metrics logging\n",
    "from torcheval.metrics import MulticlassAccuracy, Mean\n",
    "\n",
    "# for progress bar visualization\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from plinio.methods import MPS\n",
    "from plinio.methods.mps import get_default_qinfo\n",
    "from plinio.methods.mps.quant.quantizers import PACTActSigned\n",
    "from plinio.methods.mps.quant.backends import Backend, integerize_arch\n",
    "from plinio.methods.mps.quant.backends.onnx import ONNXExporter\n",
    "\n",
    "from utils.train import set_seed\n",
    "from utils.plot import plot_learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74",
   "metadata": {
    "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74"
   },
   "source": [
    "Next, we define training configurations and set basic options and paths. Note that these are not state-of-the-art settings for the CIFAR-10 dataset, and superior accuracy results can be obtained with more advanced training recipes. However, the goal is to keep this notebook as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
    "outputId": "57111746-8c05-4ced-dc9d-c4f2cad614fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    'val_split': 0.2,           # validation split\n",
    "    'float_epochs': 100,        # max epochs for floating point training\n",
    "    'qat_epochs': 100,          # max epochs for QAT\n",
    "    'batch_size': 32,           # batch size\n",
    "    'float_lr': 0.01,           # initial learning rate for floating point training\n",
    "    'qat_lr': 0.01,             # learning rate for QAT\n",
    "    'weight_decay': 1e-4,       # weight decay\n",
    "    'float_patience': 10,       # early-stopping patience for floating point training\n",
    "    'qat_patience': 10,         # early-stopping patience for QAT\n",
    "}\n",
    "\n",
    "DATA_DIR = Path(\"qat\")\n",
    "SAVE_DIR = DATA_DIR / \"local_checkpoints\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4",
   "metadata": {
    "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4"
   },
   "source": [
    "## 2) Dataset and Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef",
   "metadata": {
    "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef"
   },
   "source": [
    "Next, we download the CIFAR-10 dataset and create dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
    "outputId": "328ed0a9-884b-4ff4-9d16-0357ba0a98d4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR / \"data\",\n",
    "                                                 train=True, download=True, transform=transform_train)\n",
    "val_len = int(TRAINING_CONFIG['val_split'] * len(train_val_dataset))\n",
    "train_len = len(train_val_dataset) - val_len\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR / \"data\",\n",
    "                                            train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f8c3b-a3b1-425b-9af9-bca3744aae1f",
   "metadata": {},
   "source": [
    "And define a simple mini-ResNet CNN model. \n",
    "\n",
    "**NOTE**: This is pure PyTorch code, no modifications are required by PLiNIO at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382d393d-13fd-46ae-a161-3fe228bf372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MiniResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MiniResNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2 = BasicBlock(32, 64, stride=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=8, stride=8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb1dfc-ca7e-4c20-ad5a-59b14dc2f50e",
   "metadata": {},
   "source": [
    "Create an instance of the model and print its structure with `torchinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8782138e-a15b-48e6-ad19-6f8b2dbbdaa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MiniResNet                               [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [1, 16, 32, 32]           432\n",
      "├─BatchNorm2d: 1-2                       [1, 16, 32, 32]           32\n",
      "├─ReLU: 1-3                              [1, 16, 32, 32]           --\n",
      "├─BasicBlock: 1-4                        [1, 32, 16, 16]           --\n",
      "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           4,608\n",
      "│    └─BatchNorm2d: 2-2                  [1, 32, 16, 16]           64\n",
      "│    └─ReLU: 2-3                         [1, 32, 16, 16]           --\n",
      "│    └─Conv2d: 2-4                       [1, 32, 16, 16]           9,216\n",
      "│    └─BatchNorm2d: 2-5                  [1, 32, 16, 16]           64\n",
      "│    └─Sequential: 2-6                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 32, 16, 16]           512\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 32, 16, 16]           64\n",
      "│    └─ReLU: 2-7                         [1, 32, 16, 16]           --\n",
      "├─BasicBlock: 1-5                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-8                       [1, 64, 8, 8]             18,432\n",
      "│    └─BatchNorm2d: 2-9                  [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-10                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-11                      [1, 64, 8, 8]             36,864\n",
      "│    └─BatchNorm2d: 2-12                 [1, 64, 8, 8]             128\n",
      "│    └─Sequential: 2-13                  [1, 64, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-3                  [1, 64, 8, 8]             2,048\n",
      "│    │    └─BatchNorm2d: 3-4             [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-14                        [1, 64, 8, 8]             --\n",
      "├─MaxPool2d: 1-6                         [1, 64, 1, 1]             --\n",
      "├─Linear: 1-7                            [1, 10]                   650\n",
      "==========================================================================================\n",
      "Total params: 73,370\n",
      "Trainable params: 73,370\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 7.78\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.85\n",
      "Params size (MB): 0.29\n",
      "Estimated Total Size (MB): 1.16\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = MiniResNet().to(device)\n",
    "\n",
    "# compute the shape of a single DNN input\n",
    "input_shape = train_dataset[0][0].numpy().shape\n",
    "# show the network summary (requires a 1-input batch)\n",
    "print(summary(model, (1,) + input_shape, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cFj-TvcpcS6",
   "metadata": {
    "id": "0cFj-TvcpcS6"
   },
   "source": [
    "## 3) Floating-point Training (Optional)\n",
    "\n",
    "**NOTE**: Again, this is just (very basic) PyTorch training code. If you want to jump directly to PLiNIO optimizations (Part 3), you can skip this part and simply load a pre-cooked checkpoint with this line.\n",
    "\n",
    "```python\n",
    "model.load_state_dict(torch.load(\"./qat/checkpoints/float_best.pt\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q7rkOr7vwk9r",
   "metadata": {
    "id": "Q7rkOr7vwk9r"
   },
   "source": [
    "QAT works better starting from a pre-trained model. So, let's start by defining a simple ResNet8 CNN and training it on a few epochs on CIFAR-10. Alternatively, you can download pre-trained weights (e.g. on ImageNet), possibly fine-tuning them for a few epochs on CIFAR-10.\n",
    "\n",
    "Let's define a simple training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c874df7-dafe-4fc9-8502-adf0e2bde4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model for one epoch\n",
    "def train_one_epoch(epoch, model, criterion, optimizer, data, device):\n",
    "    model.train()\n",
    "    loss_metric = Mean()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=10)\n",
    "    loop = tqdm(data, desc=f\"Epoch {epoch+1}\")\n",
    "    for i, (images, labels) in enumerate(loop):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass and weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # log metrics\n",
    "        loss_metric.update(loss.detach().cpu())\n",
    "        acc_metric.update(outputs.detach().cpu(), labels.detach().cpu())\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = loss_metric.compute().item()\n",
    "            avg_acc = acc_metric.compute().item() * 100\n",
    "            loop.set_postfix(loss=avg_loss, accuracy=avg_acc)\n",
    "    final_metrics = {\n",
    "        'loss': loss_metric.compute().item(),\n",
    "        'acc': acc_metric.compute().item() * 100,\n",
    "    }\n",
    "    return final_metrics\n",
    "\n",
    "# function to evaluate the model for one epoch\n",
    "def evaluate(model, criterion, data, device):\n",
    "    model.eval()\n",
    "    loss_metric = Mean()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=10)\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(data, desc=f\"Evaluation\")\n",
    "        for i, (images, labels) in enumerate(loop):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_metric.update(loss.detach().cpu())\n",
    "            acc_metric.update(outputs.cpu(), labels.cpu())\n",
    "            if i == len(data) - 1:\n",
    "                avg_loss = loss_metric.compute().item()\n",
    "                avg_acc = acc_metric.compute().item() * 100\n",
    "                final_metrics = {\n",
    "                    'loss': avg_loss,\n",
    "                    'acc': avg_acc,\n",
    "                }\n",
    "                loop.set_postfix(loss=avg_loss, accuracy=avg_acc)\n",
    "    return final_metrics\n",
    "\n",
    "# global training loop\n",
    "def train(checkpoint_path, patience, epochs, model, criterion, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    # Early stopping and checkpointing\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        metrics = train_one_epoch(epoch, model, criterion, optimizer, train_loader, device)\n",
    "        val_metrics = evaluate(model, criterion, val_loader, device)\n",
    "        # rename val metrics to avoid clash\n",
    "        val_metrics = {'val_' + k: v for k, v in val_metrics.items()}\n",
    "        metrics.update(val_metrics)\n",
    "        history.append(metrics)\n",
    "\n",
    "        # Check for improvement\n",
    "        if metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = metrics['val_loss']\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Saved new best model.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf961a4-03a2-40cf-8ff8-9048899c479a",
   "metadata": {},
   "source": [
    "Run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a005e7-400f-414b-9517-0683a5f03424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1250/1250 [00:07<00:00, 163.46it/s, accuracy=35.2, loss=1.75]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 222.07it/s, accuracy=43.3, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1250/1250 [00:07<00:00, 164.84it/s, accuracy=47.1, loss=1.45]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 222.87it/s, accuracy=52.1, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1250/1250 [00:07<00:00, 166.42it/s, accuracy=52.6, loss=1.31]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.80it/s, accuracy=56.4, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1250/1250 [00:07<00:00, 166.31it/s, accuracy=56.9, loss=1.2] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.03it/s, accuracy=59.9, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1250/1250 [00:07<00:00, 167.17it/s, accuracy=60.3, loss=1.12]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.66it/s, accuracy=62.2, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1250/1250 [00:07<00:00, 170.88it/s, accuracy=62.9, loss=1.05]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 216.92it/s, accuracy=62.8, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1250/1250 [00:07<00:00, 170.29it/s, accuracy=64.9, loss=0.994]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 205.87it/s, accuracy=64.4, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1250/1250 [00:07<00:00, 166.43it/s, accuracy=66.6, loss=0.948]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 202.07it/s, accuracy=67.4, loss=0.921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1250/1250 [00:07<00:00, 168.03it/s, accuracy=68, loss=0.904]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 210.29it/s, accuracy=67.8, loss=0.909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1250/1250 [00:07<00:00, 166.26it/s, accuracy=69, loss=0.879]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.29it/s, accuracy=69.7, loss=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1250/1250 [00:07<00:00, 170.00it/s, accuracy=70.4, loss=0.838]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.55it/s, accuracy=71.2, loss=0.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1250/1250 [00:08<00:00, 147.63it/s, accuracy=71.3, loss=0.813]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.42it/s, accuracy=69.8, loss=0.865]\n",
      "Epoch 13: 100%|██████████| 1250/1250 [00:07<00:00, 171.17it/s, accuracy=72.4, loss=0.794]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 223.55it/s, accuracy=72.8, loss=0.777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1250/1250 [00:07<00:00, 171.20it/s, accuracy=73.5, loss=0.76] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.76it/s, accuracy=73, loss=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1250/1250 [00:07<00:00, 170.08it/s, accuracy=74, loss=0.739]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.13it/s, accuracy=72.5, loss=0.782]\n",
      "Epoch 16: 100%|██████████| 1250/1250 [00:07<00:00, 168.22it/s, accuracy=74.7, loss=0.727]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.88it/s, accuracy=73.5, loss=0.749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1250/1250 [00:07<00:00, 168.80it/s, accuracy=75.3, loss=0.703]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 217.31it/s, accuracy=73.6, loss=0.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1250/1250 [00:07<00:00, 167.34it/s, accuracy=75.8, loss=0.698]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.33it/s, accuracy=73.4, loss=0.753]\n",
      "Epoch 19: 100%|██████████| 1250/1250 [00:07<00:00, 170.61it/s, accuracy=76.2, loss=0.682]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.70it/s, accuracy=75.3, loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1250/1250 [00:07<00:00, 169.00it/s, accuracy=76.8, loss=0.662]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.23it/s, accuracy=75.7, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 1250/1250 [00:07<00:00, 169.32it/s, accuracy=77.4, loss=0.652]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.14it/s, accuracy=76.6, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 1250/1250 [00:07<00:00, 170.09it/s, accuracy=77.6, loss=0.644]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.03it/s, accuracy=76.2, loss=0.672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 1250/1250 [00:07<00:00, 170.75it/s, accuracy=78.2, loss=0.634]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 200.93it/s, accuracy=77.2, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 1250/1250 [00:07<00:00, 167.43it/s, accuracy=78.5, loss=0.617]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.07it/s, accuracy=76.9, loss=0.658]\n",
      "Epoch 25: 100%|██████████| 1250/1250 [00:07<00:00, 164.06it/s, accuracy=78.5, loss=0.611]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.03it/s, accuracy=77.4, loss=0.659]\n",
      "Epoch 26: 100%|██████████| 1250/1250 [00:07<00:00, 166.93it/s, accuracy=79.1, loss=0.6]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.61it/s, accuracy=76.4, loss=0.679]\n",
      "Epoch 27: 100%|██████████| 1250/1250 [00:07<00:00, 170.80it/s, accuracy=78.9, loss=0.602]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 223.44it/s, accuracy=77.6, loss=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 1250/1250 [00:07<00:00, 171.31it/s, accuracy=79.4, loss=0.592]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 230.35it/s, accuracy=77.6, loss=0.649]\n",
      "Epoch 29: 100%|██████████| 1250/1250 [00:07<00:00, 166.23it/s, accuracy=79.8, loss=0.581]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 214.16it/s, accuracy=78.5, loss=0.614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 1250/1250 [00:07<00:00, 171.27it/s, accuracy=80.3, loss=0.571]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.35it/s, accuracy=78.2, loss=0.633]\n",
      "Epoch 31: 100%|██████████| 1250/1250 [00:07<00:00, 171.81it/s, accuracy=80.4, loss=0.566]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.82it/s, accuracy=77.8, loss=0.645]\n",
      "Epoch 32: 100%|██████████| 1250/1250 [00:07<00:00, 170.60it/s, accuracy=80.7, loss=0.557]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.30it/s, accuracy=78.3, loss=0.62]\n",
      "Epoch 33: 100%|██████████| 1250/1250 [00:07<00:00, 162.01it/s, accuracy=80.6, loss=0.556]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 209.50it/s, accuracy=78.4, loss=0.625]\n",
      "Epoch 34: 100%|██████████| 1250/1250 [00:07<00:00, 167.24it/s, accuracy=81.1, loss=0.55] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.63it/s, accuracy=77.8, loss=0.643]\n",
      "Epoch 35: 100%|██████████| 1250/1250 [00:07<00:00, 165.88it/s, accuracy=81.1, loss=0.545]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.44it/s, accuracy=78.9, loss=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 1250/1250 [00:07<00:00, 169.99it/s, accuracy=81.2, loss=0.54] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 231.06it/s, accuracy=78.5, loss=0.63]\n",
      "Epoch 37: 100%|██████████| 1250/1250 [00:08<00:00, 156.17it/s, accuracy=81.3, loss=0.539]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.69it/s, accuracy=78.8, loss=0.614]\n",
      "Epoch 38: 100%|██████████| 1250/1250 [00:07<00:00, 168.84it/s, accuracy=81.7, loss=0.529]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 213.60it/s, accuracy=79.1, loss=0.598]\n",
      "Epoch 39: 100%|██████████| 1250/1250 [00:07<00:00, 168.69it/s, accuracy=82, loss=0.52]   \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.17it/s, accuracy=79.1, loss=0.607]\n",
      "Epoch 40: 100%|██████████| 1250/1250 [00:07<00:00, 171.92it/s, accuracy=82.1, loss=0.518]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.77it/s, accuracy=79.6, loss=0.589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 1250/1250 [00:07<00:00, 170.24it/s, accuracy=82.3, loss=0.515]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.18it/s, accuracy=79.2, loss=0.613]\n",
      "Epoch 42: 100%|██████████| 1250/1250 [00:07<00:00, 170.55it/s, accuracy=82.1, loss=0.514]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.89it/s, accuracy=79.3, loss=0.593]\n",
      "Epoch 43: 100%|██████████| 1250/1250 [00:07<00:00, 168.62it/s, accuracy=82.5, loss=0.504]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 230.61it/s, accuracy=79.2, loss=0.602]\n",
      "Epoch 44: 100%|██████████| 1250/1250 [00:07<00:00, 161.78it/s, accuracy=82.5, loss=0.502]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 221.72it/s, accuracy=79.2, loss=0.602]\n",
      "Epoch 45: 100%|██████████| 1250/1250 [00:07<00:00, 164.81it/s, accuracy=82.6, loss=0.499]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 217.34it/s, accuracy=79.4, loss=0.601]\n",
      "Epoch 46: 100%|██████████| 1250/1250 [00:07<00:00, 158.52it/s, accuracy=83, loss=0.491]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.09it/s, accuracy=79.4, loss=0.599]\n",
      "Epoch 47: 100%|██████████| 1250/1250 [00:07<00:00, 170.36it/s, accuracy=83.1, loss=0.49] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.06it/s, accuracy=79.7, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 1250/1250 [00:07<00:00, 170.19it/s, accuracy=83.2, loss=0.49]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.77it/s, accuracy=78.7, loss=0.62]\n",
      "Epoch 49: 100%|██████████| 1250/1250 [00:07<00:00, 169.60it/s, accuracy=82.9, loss=0.491]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 220.30it/s, accuracy=80.4, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|██████████| 1250/1250 [00:07<00:00, 170.94it/s, accuracy=83.2, loss=0.483]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.50it/s, accuracy=79.2, loss=0.605]\n",
      "Epoch 51: 100%|██████████| 1250/1250 [00:07<00:00, 168.91it/s, accuracy=83.1, loss=0.484]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 203.56it/s, accuracy=79.7, loss=0.586]\n",
      "Epoch 52: 100%|██████████| 1250/1250 [00:07<00:00, 169.29it/s, accuracy=83.6, loss=0.472]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 209.19it/s, accuracy=79.9, loss=0.598]\n",
      "Epoch 53: 100%|██████████| 1250/1250 [00:07<00:00, 165.90it/s, accuracy=83.5, loss=0.473]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.15it/s, accuracy=80.4, loss=0.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|██████████| 1250/1250 [00:07<00:00, 169.93it/s, accuracy=83.7, loss=0.468]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.36it/s, accuracy=79.8, loss=0.587]\n",
      "Epoch 55: 100%|██████████| 1250/1250 [00:07<00:00, 167.22it/s, accuracy=83.9, loss=0.467]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.16it/s, accuracy=80.6, loss=0.569]\n",
      "Epoch 56: 100%|██████████| 1250/1250 [00:08<00:00, 151.50it/s, accuracy=83.9, loss=0.463]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 222.32it/s, accuracy=80.2, loss=0.582]\n",
      "Epoch 57: 100%|██████████| 1250/1250 [00:07<00:00, 164.52it/s, accuracy=84.3, loss=0.455]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.94it/s, accuracy=80.2, loss=0.572]\n",
      "Epoch 58: 100%|██████████| 1250/1250 [00:07<00:00, 171.10it/s, accuracy=84, loss=0.465]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.43it/s, accuracy=80.7, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|██████████| 1250/1250 [00:07<00:00, 167.58it/s, accuracy=84.1, loss=0.458]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 223.17it/s, accuracy=80.6, loss=0.571]\n",
      "Epoch 60: 100%|██████████| 1250/1250 [00:07<00:00, 168.34it/s, accuracy=84.2, loss=0.452]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.75it/s, accuracy=80.4, loss=0.576]\n",
      "Epoch 61: 100%|██████████| 1250/1250 [00:07<00:00, 167.17it/s, accuracy=84.2, loss=0.454]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 210.29it/s, accuracy=81.2, loss=0.552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 1250/1250 [00:07<00:00, 168.54it/s, accuracy=84.5, loss=0.445]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.78it/s, accuracy=80.4, loss=0.577]\n",
      "Epoch 63: 100%|██████████| 1250/1250 [00:07<00:00, 170.44it/s, accuracy=84.4, loss=0.452]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 231.64it/s, accuracy=80.4, loss=0.569]\n",
      "Epoch 64: 100%|██████████| 1250/1250 [00:07<00:00, 171.14it/s, accuracy=84.7, loss=0.441]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.77it/s, accuracy=80.4, loss=0.57]\n",
      "Epoch 65: 100%|██████████| 1250/1250 [00:07<00:00, 171.87it/s, accuracy=84.4, loss=0.448]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.84it/s, accuracy=81.3, loss=0.554]\n",
      "Epoch 66: 100%|██████████| 1250/1250 [00:07<00:00, 172.03it/s, accuracy=84.9, loss=0.437]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.68it/s, accuracy=80.6, loss=0.557]\n",
      "Epoch 67: 100%|██████████| 1250/1250 [00:07<00:00, 165.98it/s, accuracy=84.8, loss=0.438]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 231.31it/s, accuracy=81.1, loss=0.553]\n",
      "Epoch 68: 100%|██████████| 1250/1250 [00:07<00:00, 158.27it/s, accuracy=84.9, loss=0.434]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.29it/s, accuracy=80.8, loss=0.554]\n",
      "Epoch 69: 100%|██████████| 1250/1250 [00:07<00:00, 167.67it/s, accuracy=85, loss=0.436]  \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.12it/s, accuracy=81, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 1250/1250 [00:07<00:00, 162.30it/s, accuracy=85, loss=0.432] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.02it/s, accuracy=81.3, loss=0.56]\n",
      "Epoch 71: 100%|██████████| 1250/1250 [00:07<00:00, 159.12it/s, accuracy=85.1, loss=0.428]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.58it/s, accuracy=81.5, loss=0.548]\n",
      "Epoch 72: 100%|██████████| 1250/1250 [00:08<00:00, 154.66it/s, accuracy=85.3, loss=0.423]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.30it/s, accuracy=81.5, loss=0.548]\n",
      "Epoch 73: 100%|██████████| 1250/1250 [00:07<00:00, 169.56it/s, accuracy=85.4, loss=0.423]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.84it/s, accuracy=81.2, loss=0.552]\n",
      "Epoch 74: 100%|██████████| 1250/1250 [00:07<00:00, 173.05it/s, accuracy=85.2, loss=0.426]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 221.76it/s, accuracy=81.2, loss=0.544]\n",
      "Epoch 75: 100%|██████████| 1250/1250 [00:07<00:00, 165.92it/s, accuracy=85.5, loss=0.418]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 230.26it/s, accuracy=81.8, loss=0.542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 1250/1250 [00:07<00:00, 162.88it/s, accuracy=85.2, loss=0.428]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.55it/s, accuracy=81.5, loss=0.544]\n",
      "Epoch 77: 100%|██████████| 1250/1250 [00:07<00:00, 170.03it/s, accuracy=85.5, loss=0.416]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 226.88it/s, accuracy=81.4, loss=0.55]\n",
      "Epoch 78: 100%|██████████| 1250/1250 [00:07<00:00, 170.96it/s, accuracy=85.4, loss=0.421]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.09it/s, accuracy=81.2, loss=0.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 1250/1250 [00:07<00:00, 170.31it/s, accuracy=85.4, loss=0.415]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 210.70it/s, accuracy=81.1, loss=0.564]\n",
      "Epoch 80: 100%|██████████| 1250/1250 [00:08<00:00, 155.79it/s, accuracy=85.9, loss=0.412]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 225.28it/s, accuracy=81.5, loss=0.552]\n",
      "Epoch 81: 100%|██████████| 1250/1250 [00:07<00:00, 166.31it/s, accuracy=85.5, loss=0.412]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.74it/s, accuracy=80.9, loss=0.56]\n",
      "Epoch 82: 100%|██████████| 1250/1250 [00:07<00:00, 171.26it/s, accuracy=85.7, loss=0.411]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.78it/s, accuracy=81.2, loss=0.561]\n",
      "Epoch 83: 100%|██████████| 1250/1250 [00:07<00:00, 170.45it/s, accuracy=85.8, loss=0.408]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.43it/s, accuracy=81.7, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 1250/1250 [00:07<00:00, 172.11it/s, accuracy=85.8, loss=0.406]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.62it/s, accuracy=81.7, loss=0.542]\n",
      "Epoch 85: 100%|██████████| 1250/1250 [00:07<00:00, 170.48it/s, accuracy=85.9, loss=0.406]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.72it/s, accuracy=81.5, loss=0.542]\n",
      "Epoch 86: 100%|██████████| 1250/1250 [00:07<00:00, 170.66it/s, accuracy=85.9, loss=0.403]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 227.62it/s, accuracy=81.2, loss=0.554]\n",
      "Epoch 87: 100%|██████████| 1250/1250 [00:07<00:00, 172.69it/s, accuracy=86.2, loss=0.396]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 230.56it/s, accuracy=81.5, loss=0.543]\n",
      "Epoch 88: 100%|██████████| 1250/1250 [00:07<00:00, 170.77it/s, accuracy=86.2, loss=0.399]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 229.26it/s, accuracy=81.8, loss=0.54]\n",
      "Epoch 89: 100%|██████████| 1250/1250 [00:07<00:00, 169.09it/s, accuracy=86, loss=0.403] \n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.24it/s, accuracy=81.3, loss=0.549]\n",
      "Epoch 90: 100%|██████████| 1250/1250 [00:07<00:00, 165.49it/s, accuracy=86.3, loss=0.398]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 224.20it/s, accuracy=81.1, loss=0.559]\n",
      "Epoch 91: 100%|██████████| 1250/1250 [00:07<00:00, 164.99it/s, accuracy=86.4, loss=0.388]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 221.70it/s, accuracy=81, loss=0.571]\n",
      "Epoch 92: 100%|██████████| 1250/1250 [00:07<00:00, 166.55it/s, accuracy=86.4, loss=0.389]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.06it/s, accuracy=81, loss=0.574]\n",
      "Epoch 93: 100%|██████████| 1250/1250 [00:07<00:00, 167.29it/s, accuracy=86.1, loss=0.397]\n",
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 228.28it/s, accuracy=81, loss=0.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 93 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = SAVE_DIR / \"float_best.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=TRAINING_CONFIG['float_lr'], weight_decay=TRAINING_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_CONFIG['float_epochs'])\n",
    "\n",
    "history = train(checkpoint_path, TRAINING_CONFIG['float_patience'], TRAINING_CONFIG['float_epochs'], model, criterion, optimizer, scheduler,\n",
    "      train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e44606-5c3f-43d8-9ed2-4d8cabd7c61a",
   "metadata": {},
   "source": [
    "Load and evaluate the best model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa1f22f-de21-4e8f-afca-c5d7897aecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 313/313 [00:00<00:00, 334.32it/s, accuracy=82, loss=0.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.97, Loss: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "test_metrics = evaluate(model, criterion, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_metrics['acc']:.2f}, Loss: {test_metrics['loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20810a56-8a4a-4fdb-b5f1-617e79c29cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rp1zVH50-atb",
   "metadata": {
    "id": "rp1zVH50-atb"
   },
   "source": [
    "## 4) Prepare the Model for QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fx2HOJgy-lBQ",
   "metadata": {
    "id": "Fx2HOJgy-lBQ"
   },
   "source": [
    "To prepare the model for QAT, it suffices to pass it to the PliNIO optimization method constructor (`MPS` in this example). The constructor internally implements the necessary conversion steps, adding \"fake-quantization\" operations to all Conv. and Linear layers. \n",
    "\n",
    "The constructor takes three main parameters:\n",
    "- The previously created `nn.Module` of the \"seed\" DNN (better if already pre-trained)\n",
    "- The shape of a single input sample (needed for internal graph analysis passes with `torch.fx`).\n",
    "- A `qinfo` dictionary containing quantization configurations (type of quantizer, bit-width, optional arguments, etc, for each weight or activation tensor of the network).\n",
    "\n",
    "Since defining `qinfo` from scratch could be annoying, we provide a utility function to generate a sane default, given only the desired weights and activations bit-widths to use. In this case, let's use 8-bit for both weights and activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6d4281-bb2c-4b2d-9ad9-328eb0221354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default qinfo dictionary, specifying 8-bit as the only precision for both weights and activations\n",
    "qinfo = get_default_qinfo((8,), (8,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be53ed-6916-4e00-9e41-32d970004226",
   "metadata": {},
   "source": [
    "The `qinfo` dictionary has two default keys:\n",
    "- `input_default` specifies the default quantizer for all input tensors.\n",
    "- `layer_default` specifies the default quantizer for all weights and activations tensors of supported layers (currently Linear and Conv).\n",
    "\n",
    "You can override these settings for each specific layer (or input) of your DNN by adding keys with the corresponding torch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee677ca-e8c3-4c7b-9026-a3d1e726d26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['layer_default', 'input_default'])\n"
     ]
    }
   ],
   "source": [
    "print(qinfo.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1644e8b-1a55-4f74-b5f3-933cdd523c56",
   "metadata": {},
   "source": [
    "You may want to customize the quantizer for input tensors, depending on the nature of your training data. For example, as we have normalized CIFAR-10 images roughly to the $[-1,1]$ range with `transforms.Normalize` at the beginning of this notebook, we probably want our input quantizer to map the entirety of that range to the available integer values ($[-128:127]$ for 8-bit). To do so, we simply have to overwrite the dictionary entries corresponding to the `input_default` key. Namely:\n",
    "- We set the input quantizer to `PACTActSigned`, a signed version of [PACT](https://arxiv.org/abs/1805.06085) with two trainable thresholds\n",
    "- We initialize such two thresholds (called `init_clip_val_inf/sup` in PLiNIO's implementation) to -1 and +1 respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d34d4e7-a887-49ad-833f-ae4745a701a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qinfo['input_default']['quantizer'] = PACTActSigned\n",
    "qinfo['input_default']['kwargs'] = {'init_clip_val_inf': -1, 'init_clip_val_sup': +1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334f897-e35d-4a43-aadb-294edc75563d",
   "metadata": {},
   "source": [
    "We are now ready to call the `MPS` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VImIBMbW-s32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "VImIBMbW-s32",
    "outputId": "a1df1470-78b1-43cb-f65a-32073dc9558e"
   },
   "outputs": [],
   "source": [
    "mps_model = MPS(copy.deepcopy(model), input_shape=input_shape, qinfo=qinfo)\n",
    "mps_model = mps_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431859b4-0a62-4718-b6ea-28cea7e0ab9f",
   "metadata": {},
   "source": [
    "**NOTE**: as most other PLiNIO methods, `MPS` supports passing a **cost model** to the constructor, to specify non-functional optimization metric(s), e.g., model size, latency, energy, etc. However, since this example refers to a single-precision QAT (so both the network architecture and the precision are fixed), PLiNIO has *no freedom to optimize cost*, and the cost model is irrelevant.\n",
    "\n",
    "In other words, after exporting the model and compiling it for edge inference, its size will be reduced by approximately 4x moving from float32 to int8, and latency will also most probably improve thanks to the usage of integer arithmetics. However, PLiNIO has no way to *alter the cost vs accuracy trade-off at training time* since the only available option is 8-bit quantization. \n",
    "\n",
    "Therefore, in this case, we can omit the cost model from the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57069995-37d2-4952-a391-bbf8526b5d65",
   "metadata": {},
   "source": [
    "### Evaluating the Converted Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f968c0a-5ad1-4508-814b-32f332e6d1db",
   "metadata": {},
   "source": [
    "The model generated by the MPS constructor has fake-quantization operations to simulate `int8` precision. Furthermore, other optimizations are performed during the conversion, such as folding Batch Normalization layers with Convolutions or Linear layers. Overall, the result of the conversion is similar to what we would get with a (very basic) Post-Training Quantization (PTQ). Let's check how this model performs on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2674479d-a358-4a30-993a-4f44fe6488ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 101.04it/s, accuracy=73.2, loss=0.78]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7803315492674184, Test Acc: 73.15999865531921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_metrics = evaluate(mps_model, criterion, test_loader, device)\n",
    "print(f'Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1532046-d167-49d7-b598-b47b2efc9c3d",
   "metadata": {},
   "source": [
    "As you can see, the performance drops significantly. Note that this is due to PLiNIO's MPS initialization currently not being particularly smart (e.g., all activation quantizers' initial ranges are set to the same default value, rather than being tuned to each tensor's distribution). However, QAT can usually recover this drop in a few epochs. Let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PI6CEP-PJdnO",
   "metadata": {
    "id": "PI6CEP-PJdnO"
   },
   "source": [
    "## 5) Running the Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qb8qLFW8Bti7",
   "metadata": {
    "id": "Qb8qLFW8Bti7"
   },
   "source": [
    "We are now ready to run QAT loop. Note that, if we wanted to actually *select* the bitwidth using MPS, we would have to implement something more similar to the `prune()` function in [this](https://github.com/eml-eda/plinio/blob/main/examples/channel_pruning_pit.ipynb) notebook (which exemplifies the PIT channel-pruning method). However, we're keeping a fixed precision, as discussed above. So, in this case, we can *entirely reuse* the `train()` function defined above, just passing the `mps_model` to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc6e5518-e610-4e4d-aaeb-2169eb0a462b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1250/1250 [00:20<00:00, 62.19it/s, accuracy=81.1, loss=0.541]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.10it/s, accuracy=78.5, loss=0.632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1250/1250 [00:21<00:00, 57.59it/s, accuracy=83.2, loss=0.485]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 87.98it/s, accuracy=80.2, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1250/1250 [00:18<00:00, 67.98it/s, accuracy=83.7, loss=0.468]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.17it/s, accuracy=79, loss=0.637]\n",
      "Epoch 4: 100%|██████████| 1250/1250 [00:18<00:00, 68.74it/s, accuracy=83.9, loss=0.464]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.38it/s, accuracy=79.9, loss=0.594]\n",
      "Epoch 5: 100%|██████████| 1250/1250 [00:18<00:00, 68.71it/s, accuracy=84.2, loss=0.453]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 82.52it/s, accuracy=80, loss=0.585]\n",
      "Epoch 6: 100%|██████████| 1250/1250 [00:20<00:00, 60.02it/s, accuracy=84.5, loss=0.444]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.13it/s, accuracy=81.2, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1250/1250 [00:18<00:00, 67.18it/s, accuracy=84.7, loss=0.442]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.62it/s, accuracy=80.2, loss=0.58]\n",
      "Epoch 8: 100%|██████████| 1250/1250 [00:20<00:00, 61.79it/s, accuracy=84.8, loss=0.438]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 88.84it/s, accuracy=80, loss=0.579]\n",
      "Epoch 9: 100%|██████████| 1250/1250 [00:20<00:00, 61.23it/s, accuracy=84.8, loss=0.43] \n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.74it/s, accuracy=80.5, loss=0.585]\n",
      "Epoch 10: 100%|██████████| 1250/1250 [00:19<00:00, 62.75it/s, accuracy=85.2, loss=0.429]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.88it/s, accuracy=78.6, loss=0.65]\n",
      "Epoch 11: 100%|██████████| 1250/1250 [00:18<00:00, 68.73it/s, accuracy=85.1, loss=0.427]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 86.33it/s, accuracy=80.8, loss=0.561]\n",
      "Epoch 12: 100%|██████████| 1250/1250 [00:18<00:00, 67.06it/s, accuracy=85.1, loss=0.425]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.41it/s, accuracy=80.9, loss=0.567]\n",
      "Epoch 13: 100%|██████████| 1250/1250 [00:18<00:00, 68.26it/s, accuracy=85.6, loss=0.416]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 88.14it/s, accuracy=80.7, loss=0.586]\n",
      "Epoch 14: 100%|██████████| 1250/1250 [00:19<00:00, 64.72it/s, accuracy=85.4, loss=0.414]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 80.53it/s, accuracy=81.2, loss=0.577]\n",
      "Epoch 15: 100%|██████████| 1250/1250 [00:21<00:00, 56.84it/s, accuracy=85.9, loss=0.408]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 90.66it/s, accuracy=81.2, loss=0.568]\n",
      "Epoch 16: 100%|██████████| 1250/1250 [00:18<00:00, 68.42it/s, accuracy=85.5, loss=0.414]\n",
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 89.05it/s, accuracy=81.3, loss=0.562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 16 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = SAVE_DIR / \"qat_best.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mps_model.parameters(), lr=TRAINING_CONFIG['qat_lr'], weight_decay=TRAINING_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_CONFIG['qat_epochs'])\n",
    "\n",
    "history = train(checkpoint_path, TRAINING_CONFIG['qat_patience'], TRAINING_CONFIG['qat_epochs'], mps_model, criterion, optimizer, scheduler,\n",
    "      train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead6beb-08f8-45aa-9188-60c02744a75f",
   "metadata": {},
   "source": [
    "The QAT should terminate after few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7b18a-7fa9-42c8-9513-758943e397ea",
   "metadata": {
    "id": "2ee795d5-cadb-45fe-92db-1cceb1638ffb"
   },
   "source": [
    "### Evaluating the Fake-quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6c8bc-85a5-42fb-8099-d916ef2b7274",
   "metadata": {},
   "source": [
    "Let's check the test accuracy of the fake-quantized DNN after QAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7625e43f-b8d0-462c-85a0-af8871fc04b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4089ce-bfe8-4768-8456-e44d6c37b684",
    "outputId": "279110db-8205-4391-b1c6-c14bd11df777",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 313/313 [00:03<00:00, 100.89it/s, accuracy=81.2, loss=0.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Model Test Loss: 0.549550117347568, Test Acc: 81.22000098228455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mps_model.load_state_dict(torch.load(checkpoint_path))\n",
    "mps_model.eval()\n",
    "\n",
    "test_metrics = evaluate(mps_model, criterion, test_loader, device)\n",
    "print(f'Exported Model Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344c2c-083c-4470-b594-1d69d5715fce",
   "metadata": {},
   "source": [
    "The model should have now recovered the same validation accuracy of the float version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aRJOELwKSP2",
   "metadata": {
    "id": "6aRJOELwKSP2"
   },
   "source": [
    "## 6) Final Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lXI9Y6lKfhF",
   "metadata": {
    "id": "7lXI9Y6lKfhF"
   },
   "source": [
    "We can now call the `.export()` method of the PLiNIO MPS model. For other PLiNIO methods (SuperNet, PIT, etc) `.export()` returns a vanilla Torch model. However, in the case of MPS, we need to preserve quantization information, and the corresponding parameters. Therefore, exporting the model causes each quantized layer to be replaced with an instance of a custom quantized class (for instance, `nn.Conv2D` becomes `QuantConv2D`). These layers function analogously to the torch equivalents, but also store the quantization parameters (e.g. min/max values for each weight tensor), and use them to simulate the effect of quantization during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "IfBE26P9EvzI",
   "metadata": {
    "id": "IfBE26P9EvzI"
   },
   "outputs": [],
   "source": [
    "quant_model = mps_model.export()\n",
    "quant_model = quant_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Ahd0um3Kxkt",
   "metadata": {
    "id": "3Ahd0um3Kxkt"
   },
   "source": [
    "Let's look at the exported model using `torchinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "MhDpsvm8K0b0",
   "metadata": {
    "id": "MhDpsvm8K0b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MiniResNet                               [1, 10]                   --\n",
      "├─QuantIdentity: 1-1                     [1, 3, 32, 32]            2\n",
      "├─QuantConv2d: 1-2                       --                        (recursive)\n",
      "│    └─PACTActSigned: 2-1                [1, 3, 32, 32]            2\n",
      "├─QuantConv2d: 1-3                       [1, 16, 32, 32]           451\n",
      "│    └─MinMaxWeight: 2-2                 [16, 3, 3, 3]             --\n",
      "│    └─QuantizerBias: 2-3                [16]                      --\n",
      "├─Module: 1-12                           --                        (recursive)\n",
      "│    └─Module: 2-12                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-5             --                        (recursive)\n",
      "│    │    │    └─PACTAct: 4-1            [1, 16, 32, 32]           1\n",
      "├─ReLU: 1-5                              [1, 16, 32, 32]           --\n",
      "├─Module: 1-12                           --                        (recursive)\n",
      "│    └─QuantConv2d: 2-5                  [1, 32, 16, 16]           4,642\n",
      "│    │    └─MinMaxWeight: 3-2            [32, 16, 3, 3]            --\n",
      "│    │    └─QuantizerBias: 3-3           [32]                      --\n",
      "│    └─QuantConv2d: 2-10                 --                        (recursive)\n",
      "│    │    └─PACTAct: 3-4                 [1, 32, 16, 16]           1\n",
      "│    └─ReLU: 2-7                         [1, 32, 16, 16]           --\n",
      "│    └─QuantConv2d: 2-8                  [1, 32, 16, 16]           9,250\n",
      "│    └─Module: 2-12                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-5             --                        (recursive)\n",
      "│    │    │    └─MinMaxWeight: 4-2       [32, 32, 3, 3]            --\n",
      "│    └─QuantConv2d: 2-10                 --                        (recursive)\n",
      "│    │    └─QuantizerBias: 3-6           [32]                      --\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─Module: 2-23                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-14            --                        (recursive)\n",
      "│    │    │    └─PACTAct: 4-3            [1, 32, 16, 16]           1\n",
      "├─Module: 1-12                           --                        (recursive)\n",
      "│    └─Module: 2-12                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-8             [1, 32, 16, 16]           546\n",
      "│    │    │    └─MinMaxWeight: 4-4       [32, 16, 1, 1]            --\n",
      "│    │    │    └─QuantizerBias: 4-5      [32]                      --\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─Module: 2-23                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-14            --                        (recursive)\n",
      "│    │    │    └─PACTAct: 4-6            [1, 32, 16, 16]           (recursive)\n",
      "├─QuantAdd: 1-10                         [1, 32, 16, 16]           1\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─Module: 2-23                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-14            --                        (recursive)\n",
      "│    │    │    └─PACTAct: 4-7            [1, 32, 16, 16]           (recursive)\n",
      "├─Module: 1-12                           --                        (recursive)\n",
      "│    └─ReLU: 2-15                        [1, 32, 16, 16]           --\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─QuantConv2d: 2-16                 [1, 64, 8, 8]             18,498\n",
      "│    │    └─MinMaxWeight: 3-11           [64, 32, 3, 3]            --\n",
      "│    │    └─QuantizerBias: 3-12          [64]                      --\n",
      "│    └─QuantConv2d: 2-21                 --                        (recursive)\n",
      "│    │    └─PACTAct: 3-13                [1, 64, 8, 8]             1\n",
      "│    └─ReLU: 2-18                        [1, 64, 8, 8]             --\n",
      "│    └─QuantConv2d: 2-19                 [1, 64, 8, 8]             36,930\n",
      "│    └─Module: 2-23                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-14            --                        (recursive)\n",
      "│    │    │    └─MinMaxWeight: 4-8       [64, 64, 3, 3]            --\n",
      "│    └─QuantConv2d: 2-21                 --                        (recursive)\n",
      "│    │    └─QuantizerBias: 3-15          [64]                      --\n",
      "├─QuantLinear: 1-18                      --                        (recursive)\n",
      "│    └─PACTAct: 2-22                     [1, 64, 8, 8]             1\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─Module: 2-23                      --                        (recursive)\n",
      "│    │    └─QuantConv2d: 3-16            [1, 64, 8, 8]             2,114\n",
      "│    │    │    └─MinMaxWeight: 4-9       [64, 32, 1, 1]            --\n",
      "│    │    │    └─QuantizerBias: 4-10     [64]                      --\n",
      "├─QuantLinear: 1-18                      --                        (recursive)\n",
      "│    └─PACTAct: 2-24                     [1, 64, 8, 8]             (recursive)\n",
      "├─QuantAdd: 1-17                         [1, 64, 8, 8]             1\n",
      "├─QuantLinear: 1-18                      --                        (recursive)\n",
      "│    └─PACTAct: 2-25                     [1, 64, 8, 8]             (recursive)\n",
      "├─Module: 1-19                           --                        (recursive)\n",
      "│    └─ReLU: 2-26                        [1, 64, 8, 8]             --\n",
      "├─MaxPool2d: 1-20                        [1, 64, 1, 1]             --\n",
      "├─QuantLinear: 1-21                      [1, 10]                   651\n",
      "│    └─MinMaxWeight: 2-27                [10, 64]                  --\n",
      "│    └─QuantizerBias: 2-28               [10]                      --\n",
      "│    └─DummyQuantizer: 2-29              [1, 10]                   --\n",
      "==========================================================================================\n",
      "Total params: 73,093\n",
      "Trainable params: 73,093\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.55\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.56\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(quant_model, (1,) + input_shape, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b7957-8705-429b-8746-c6f5620b782b",
   "metadata": {},
   "source": [
    "The printout shows the `Quant*` modules with their internal quantizers (`MinMaxWeight`,`QuantizerBias` and  `PACTAct` for weights, biases and activations respectively). Note that some layers do not have all quantizers as internal modules as PLiNIO automatically identifies *sharing constraints* (e.g., layers that must have the same output quantizer because their respective outputs are summed together)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff20dd8-44c4-4f0b-a32b-ebedbb0a7cc0",
   "metadata": {},
   "source": [
    "### Export to ONNX for AI Compilation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ffdc4-7c3e-4987-a95f-0a85f2993808",
   "metadata": {},
   "source": [
    "PLiNIO also supports exporting a quantized model to [ONNX](https://onnx.ai/) format, which is often used by AI compilers, especially for embedded/edge devices, for generating low-level code for the target. While the generated file follows the ONNX standard, tested compatibility with AI compilers is currently limited to [MATCH](https://arxiv.org/abs/2410.08855) (suggested) and [DORY](https://arxiv.org/abs/2008.07127) (deprecated). To generate a full-integer ONNX, we need two final steps:\n",
    "\n",
    "- Integerization, which converts the model from fake-quantized to true-integer (i.e., all parameters, including scale factors for quantizers, are rounded to integers at the target precision.\n",
    "- Backend exporting.\n",
    "\n",
    "The following cell integerizes the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6832a7d1-e838-4652-ba86-08551fd834fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the model to full-integer, compiler-compliant format\n",
    "full_int_model = integerize_arch(copy.deepcopy(quant_model).cpu(), Backend.ONNX, backend_kwargs={'shift_pos': 16, 'scale_bit': 24, \"dequantize_output\": True})\n",
    "#full_int_model = full_int_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38395620-20ab-49c0-a373-41b09bb7d1af",
   "metadata": {},
   "source": [
    "Evaluate the full integer model to verify the final accuracy (a small drop is inevitable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce546c9-588e-4469-a7c9-b8547307a02d",
   "metadata": {
    "id": "DVFyXZ7nLTky",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 313/313 [00:01<00:00, 161.74it/s, accuracy=81.4, loss=0.572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Integer Model Test Acc: 81.41000270843506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_metrics = evaluate(full_int_model, criterion, test_loader, device)\n",
    "test_metrics = evaluate(full_int_model, criterion, test_loader, 'cpu')\n",
    "print(f'Full Integer Model Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f032825-71f6-446a-941a-5de14a32856d",
   "metadata": {},
   "source": [
    "Lastly, let's generate the output ONNX and save it to the DATA_DIR folder. You can open the `.onnx` file with a tool like [Netron](https://netron.app/) to verify its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "KyC4xItHLTk0",
   "metadata": {
    "id": "KyC4xItHLTk0"
   },
   "outputs": [],
   "source": [
    "exporter = ONNXExporter()\n",
    "exporter.export(full_int_model, (1,) + input_shape, DATA_DIR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "drones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
