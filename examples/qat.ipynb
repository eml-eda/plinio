{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f978ca1",
   "metadata": {
    "id": "0f978ca1"
   },
   "source": [
    "# Quantization-Aware Training as a Corner Case of Mixed-Precision Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabf2b8-fef7-4474-9098-6eb1d53a3c4d",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3898ee-c56c-4138-9d0f-b1f843ae84fd",
   "metadata": {},
   "source": [
    "Install requirements for this and other examples as follows:\n",
    "```bash\n",
    "pip install -r <plinio_folder>/examples/requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac54ad-060e-4546-9dd9-1df9c5f5c181",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2bcfe",
   "metadata": {
    "id": "59c2bcfe"
   },
   "source": [
    "PLiNIO can be used to perform **Quantization-Aware Training (QAT)**, producing \"full integer\" models compatible with edge devices without a Floating Point Unit (FPU).  PLiNIO's QAT function is embedded in the `MPS()` class, which performs *Mixed-Precision Search*. Namely, it can apply QAT at *multiple bit-widths* simultaneously, and use a SuperNet-like method to select the best precision assignment for the weights and activations of different portions of a DNN (different layers, or even different channels of the same layer), balancing accuracy and inference cost.\n",
    "\n",
    "To implement a \"standard\" single-precision QAT, we can simply reduce it to a **\"corner case\" of MPS, with a single precision** (8-bit in this example).\n",
    "\n",
    "If you're interested in the details on the MPS algorithm present in PLiNIO, check-out these  papers: [link1](https://arxiv.org/abs/2407.01054) [link2](https://arxiv.org/abs/2004.05795) and the library documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90",
   "metadata": {
    "id": "fcf3da17-365b-4f04-86ac-a6dd0b7aac90"
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3",
   "metadata": {
    "id": "58d8e445-aa06-43d3-8ad9-059be3da58c3"
   },
   "source": [
    "We start by importing required libraries. The details of PLiNIO imports are the following:\n",
    "- The `MPS` class implements the MPS/QAT optimization method.\n",
    "- The `get_default_qinfo` function returns a dictionary containing default quantization settings.\n",
    "- The `PACTActSigned` class implements [PACT](https://arxiv.org/abs/1805.06085) activations quantization for signed data and is used to customize the quantization behaviour for our network's input (see below).\n",
    "- Imports from the `backends` sub-package are required to export an ONNX file compatible with the [MATCH](https://arxiv.org/abs/2410.08855) AI Compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
   "metadata": {
    "id": "a4a45bae-11ce-4318-9386-f74cb68ca0f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# for loss/metrics logging\n",
    "from torcheval.metrics import MulticlassAccuracy, Mean\n",
    "\n",
    "# for progress bar visualization\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from plinio.methods import MPS\n",
    "from plinio.methods.mps import get_default_qinfo\n",
    "from plinio.methods.mps.quant.quantizers import PACTActSigned\n",
    "from plinio.methods.mps.quant.backends import Backend, integerize_arch\n",
    "from plinio.methods.mps.quant.backends.onnx import ONNXExporter\n",
    "\n",
    "from utils.train import set_seed\n",
    "from utils.plot import plot_learning_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74",
   "metadata": {
    "id": "f2b9e0f1-3de5-4840-ab8a-d89176a4bd74"
   },
   "source": [
    "Next, we define training configurations and set basic options and paths. Note that these are not state-of-the-art settings for the CIFAR-10 dataset, and superior accuracy results can be obtained with more advanced training recipes. However, the goal is to keep this notebook as simple as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dddbd68-b3e8-4e35-a9c6-cfe0d12d6ade",
    "outputId": "57111746-8c05-4ced-dc9d-c4f2cad614fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    'val_split': 0.2,           # validation split\n",
    "    'float_epochs': 100,        # max epochs for floating point training\n",
    "    'qat_epochs': 100,          # max epochs for QAT\n",
    "    'batch_size': 32,           # batch size\n",
    "    'float_lr': 0.01,           # initial learning rate for floating point training\n",
    "    'qat_lr': 0.01,             # learning rate for QAT\n",
    "    'weight_decay': 1e-4,       # weight decay\n",
    "    'float_patience': 10,       # early-stopping patience for floating point training\n",
    "    'qat_patience': 10,         # early-stopping patience for QAT\n",
    "}\n",
    "\n",
    "DATA_DIR = Path(\"qat\")\n",
    "SAVE_DIR = DATA_DIR / \"local_checkpoints\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Working on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4",
   "metadata": {
    "id": "4e410210-8a49-4d48-b0b2-e7061b1c96e4"
   },
   "source": [
    "## 2) Dataset and Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef",
   "metadata": {
    "id": "3c831a8e-e2b2-4c51-a4f5-ca12ac3ec5ef"
   },
   "source": [
    "Next, we download the CIFAR-10 dataset and create dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1b7f59f-1367-4417-9d1d-f100f61d1e8f",
    "outputId": "328ed0a9-884b-4ff4-9d16-0357ba0a98d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_val_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR / \"data\",\n",
    "                                                 train=True, download=True, transform=transform_train)\n",
    "val_len = int(TRAINING_CONFIG['val_split'] * len(train_val_dataset))\n",
    "train_len = len(train_val_dataset) - val_len\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=DATA_DIR / \"data\",\n",
    "                                            train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f8c3b-a3b1-425b-9af9-bca3744aae1f",
   "metadata": {},
   "source": [
    "And define a simple mini-ResNet CNN model. \n",
    "\n",
    "**NOTE**: This is pure PyTorch code, no modifications are required by PLiNIO at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382d393d-13fd-46ae-a161-3fe228bf372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MiniResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MiniResNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2 = BasicBlock(32, 64, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb1dfc-ca7e-4c20-ad5a-59b14dc2f50e",
   "metadata": {},
   "source": [
    "Create an instance of the model and print its structure with `torchinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8782138e-a15b-48e6-ad19-6f8b2dbbdaa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MiniResNet                               [1, 10]                   --\n",
      "├─Conv2d: 1-1                            [1, 16, 32, 32]           432\n",
      "├─BatchNorm2d: 1-2                       [1, 16, 32, 32]           32\n",
      "├─ReLU: 1-3                              [1, 16, 32, 32]           --\n",
      "├─BasicBlock: 1-4                        [1, 32, 16, 16]           --\n",
      "│    └─Conv2d: 2-1                       [1, 32, 16, 16]           4,608\n",
      "│    └─BatchNorm2d: 2-2                  [1, 32, 16, 16]           64\n",
      "│    └─ReLU: 2-3                         [1, 32, 16, 16]           --\n",
      "│    └─Conv2d: 2-4                       [1, 32, 16, 16]           9,216\n",
      "│    └─BatchNorm2d: 2-5                  [1, 32, 16, 16]           64\n",
      "│    └─Sequential: 2-6                   [1, 32, 16, 16]           --\n",
      "│    │    └─Conv2d: 3-1                  [1, 32, 16, 16]           512\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 32, 16, 16]           64\n",
      "│    └─ReLU: 2-7                         [1, 32, 16, 16]           --\n",
      "├─BasicBlock: 1-5                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-8                       [1, 64, 8, 8]             18,432\n",
      "│    └─BatchNorm2d: 2-9                  [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-10                        [1, 64, 8, 8]             --\n",
      "│    └─Conv2d: 2-11                      [1, 64, 8, 8]             36,864\n",
      "│    └─BatchNorm2d: 2-12                 [1, 64, 8, 8]             128\n",
      "│    └─Sequential: 2-13                  [1, 64, 8, 8]             --\n",
      "│    │    └─Conv2d: 3-3                  [1, 64, 8, 8]             2,048\n",
      "│    │    └─BatchNorm2d: 3-4             [1, 64, 8, 8]             128\n",
      "│    └─ReLU: 2-14                        [1, 64, 8, 8]             --\n",
      "├─AdaptiveAvgPool2d: 1-6                 [1, 64, 1, 1]             --\n",
      "├─Linear: 1-7                            [1, 10]                   650\n",
      "==========================================================================================\n",
      "Total params: 73,370\n",
      "Trainable params: 73,370\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 7.78\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.85\n",
      "Params size (MB): 0.29\n",
      "Estimated Total Size (MB): 1.16\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = MiniResNet().to(device)\n",
    "\n",
    "# compute the shape of a single DNN input\n",
    "input_shape = train_dataset[0][0].numpy().shape\n",
    "# show the network summary (requires a 1-input batch)\n",
    "print(summary(model, (1,) + input_shape, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cFj-TvcpcS6",
   "metadata": {
    "id": "0cFj-TvcpcS6"
   },
   "source": [
    "## 3) Floating-point Training (Optional)\n",
    "\n",
    "**NOTE**: Again, this is just (very basic) PyTorch training code. If you want to jump directly to PLiNIO optimizations (Part 3), you can skip this part and simply load a pre-cooked checkpoint with this line.\n",
    "\n",
    "```python\n",
    "model.load_state_dict(torch.load(\"./qat/checkpoints/float_best.pt\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q7rkOr7vwk9r",
   "metadata": {
    "id": "Q7rkOr7vwk9r"
   },
   "source": [
    "QAT works better starting from a pre-trained model. So, let's start by defining a simple ResNet8 CNN and training it on a few epochs on CIFAR-10. Alternatively, you can download pre-trained weights (e.g. on ImageNet), possibly fine-tuning them for a few epochs on CIFAR-10.\n",
    "\n",
    "Let's define a simple training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c874df7-dafe-4fc9-8502-adf0e2bde4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model for one epoch \n",
    "def train_one_epoch(epoch, model, criterion, optimizer, data, device):\n",
    "    model.train()\n",
    "    loss_metric = Mean()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=10)\n",
    "    loop = tqdm(data, desc=f\"Epoch {epoch+1}\")\n",
    "    for i, (images, labels) in enumerate(loop):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass and weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # log metrics\n",
    "        loss_metric.update(loss.detach().cpu())\n",
    "        acc_metric.update(outputs.detach().cpu(), labels.detach().cpu())\n",
    "        if i % 100 == 99:\n",
    "            avg_loss = loss_metric.compute().item()\n",
    "            avg_acc = acc_metric.compute().item() * 100\n",
    "            loop.set_postfix(loss=avg_loss, accuracy=avg_acc)\n",
    "    final_metrics = {\n",
    "        'loss': loss_metric.compute().item(),\n",
    "        'acc': acc_metric.compute().item() * 100,\n",
    "    }\n",
    "    return final_metrics\n",
    "\n",
    "# function to evaluate the model for one epoch\n",
    "def evaluate(model, criterion, data, device):\n",
    "    model.eval()\n",
    "    loss_metric = Mean()\n",
    "    acc_metric = MulticlassAccuracy(num_classes=10)\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(data, desc=f\"Evaluation\")\n",
    "        for i, (images, labels) in enumerate(loop):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_metric.update(loss.detach().cpu())\n",
    "            acc_metric.update(outputs.cpu(), labels.cpu())\n",
    "            if i == len(data) - 1:\n",
    "                avg_loss = loss_metric.compute().item()\n",
    "                avg_acc = acc_metric.compute().item() * 100\n",
    "                final_metrics = {\n",
    "                    'loss': avg_loss,\n",
    "                    'acc': avg_acc,\n",
    "                }\n",
    "                loop.set_postfix(loss=avg_loss, accuracy=avg_acc)\n",
    "    return final_metrics\n",
    "\n",
    "# global training loop\n",
    "def train(checkpoint_path, patience, epochs, model, criterion, optimizer, scheduler, train_loader, val_loader, device):\n",
    "    # Early stopping and checkpointing\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        metrics = train_one_epoch(epoch, model, criterion, optimizer, train_loader, device)\n",
    "        val_metrics = evaluate(model, criterion, val_loader, device)\n",
    "        # rename val metrics to avoid clash\n",
    "        val_metrics = {'val_' + k: v for k, v in val_metrics.items()}\n",
    "        metrics.update(val_metrics)\n",
    "        history.append(metrics)\n",
    "\n",
    "        # Check for improvement\n",
    "        if metrics['val_loss'] < best_val_loss:\n",
    "            best_val_loss = metrics['val_loss']\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Saved new best model.\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "    return history\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf961a4-03a2-40cf-8ff8-9048899c479a",
   "metadata": {},
   "source": [
    "Run the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a005e7-400f-414b-9517-0683a5f03424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 67.15it/s, accuracy=30.7, loss=1.88]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 103.73it/s, accuracy=38.6, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.41it/s, accuracy=41.1, loss=1.61]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 104.03it/s, accuracy=43.3, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.73it/s, accuracy=47.2, loss=1.46]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 104.08it/s, accuracy=51.5, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.98it/s, accuracy=50.9, loss=1.36]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 104.05it/s, accuracy=53.2, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.35it/s, accuracy=54.5, loss=1.27]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 103.88it/s, accuracy=56.5, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 67.29it/s, accuracy=56.8, loss=1.21]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 104.04it/s, accuracy=57.5, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 67.35it/s, accuracy=58.9, loss=1.15]\n",
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 103.90it/s, accuracy=58.5, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.44it/s, accuracy=60.8, loss=1.11]\n",
      "Evaluation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 101.22it/s, accuracy=62, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 66.45it/s, accuracy=62.2, loss=1.07]\n",
      "Evaluation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 102.41it/s, accuracy=62.5, loss=1.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [00:18<00:00, 65.90it/s, accuracy=63.6, loss=1.03]\n",
      "Evaluation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:03<00:00, 102.10it/s, accuracy=62.2, loss=1.06]\n",
      "Epoch 11:  24%|██████████████████████████▊                                                                                     | 299/1250 [00:04<00:14, 66.97it/s, accuracy=64.3, loss=1]"
     ]
    }
   ],
   "source": [
    "checkpoint_path = SAVE_DIR / \"float_best.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=TRAINING_CONFIG['float_lr'], weight_decay=TRAINING_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_CONFIG['float_epochs'])\n",
    "\n",
    "history = train(checkpoint_path, TRAINING_CONFIG['float_patience'], TRAINING_CONFIG['float_epochs'], model, criterion, optimizer, scheduler,\n",
    "      train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e44606-5c3f-43d8-9ed2-4d8cabd7c61a",
   "metadata": {},
   "source": [
    "Load and evaluate the best model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1f22f-de21-4e8f-afca-c5d7897aecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "test_metrics = evaluate(model, criterion, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_metrics['acc']:.2f}, Loss: {test_metrics['loss']:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20810a56-8a4a-4fdb-b5f1-617e79c29cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rp1zVH50-atb",
   "metadata": {
    "id": "rp1zVH50-atb"
   },
   "source": [
    "## 4) Prepare the Model for QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fx2HOJgy-lBQ",
   "metadata": {
    "id": "Fx2HOJgy-lBQ"
   },
   "source": [
    "To prepare the model for QAT, it suffices to pass it to the PliNIO optimization method constructor (`MPS` in this example). The constructor internally implements the necessary conversion steps, adding \"fake-quantization\" operations to all Conv. and Linear layers. \n",
    "\n",
    "The constructor takes three main parameters:\n",
    "- The previously created `nn.Module` of the \"seed\" DNN (better if already pre-trained)\n",
    "- The shape of a single input sample (needed for internal graph analysis passes with `torch.fx`).\n",
    "- A `qinfo` dictionary containing quantization configurations (type of quantizer, bit-width, optional arguments, etc, for each weight or activation tensor of the network).\n",
    "\n",
    "Since defining `qinfo` from scratch could be annoying, we provide a utility function to generate a sane default, given only the desired weights and activations bit-widths to use. In this case, let's use 8-bit for both weights and activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d4281-bb2c-4b2d-9ad9-328eb0221354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default qinfo dictionary, specifying 8-bit as the only precision for both weights and activations\n",
    "qinfo = get_default_qinfo((8,), (8,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be53ed-6916-4e00-9e41-32d970004226",
   "metadata": {},
   "source": [
    "The `qinfo` dictionary has two default keys:\n",
    "- `input_default` specifies the default quantizer for all input tensors.\n",
    "- `layer_default` specifies the default quantizer for all weights and activations tensors of supported layers (currently Linear and Conv).\n",
    "\n",
    "You can override these settings for each specific layer (or input) of your DNN by adding keys with the corresponding torch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee677ca-e8c3-4c7b-9026-a3d1e726d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qinfo.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1644e8b-1a55-4f74-b5f3-933cdd523c56",
   "metadata": {},
   "source": [
    "You may want to customize the quantizer for input tensors, depending on the nature of your training data. For example, as we have normalized CIFAR-10 images roughly to the $[-1,1]$ range with `transforms.Normalize` at the beginning of this notebook, we probably want our input quantizer to map the entirety of that range to the available integer values ($[-128:127]$ for 8-bit). To do so, we simply have to overwrite the dictionary entries corresponding to the `input_default` key. Namely:\n",
    "- We set the input quantizer to `PACTActSigned`, a signed version of [PACT](https://arxiv.org/abs/1805.06085) with two trainable thresholds\n",
    "- We initialize such two thresholds (called `init_clip_val_inf/sup` in PLiNIO's implementation) to -1 and +1 respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34d4e7-a887-49ad-833f-ae4745a701a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qinfo['input_default']['quantizer'] = PACTActSigned\n",
    "qinfo['input_default']['kwargs'] = {'init_clip_val_inf': -1, 'init_clip_val_sup': +1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334f897-e35d-4a43-aadb-294edc75563d",
   "metadata": {},
   "source": [
    "We are now ready to call the `MPS` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VImIBMbW-s32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "VImIBMbW-s32",
    "outputId": "a1df1470-78b1-43cb-f65a-32073dc9558e"
   },
   "outputs": [],
   "source": [
    "mps_model = MPS(model, input_shape=input_shape, qinfo=qinfo)\n",
    "mps_model = mps_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431859b4-0a62-4718-b6ea-28cea7e0ab9f",
   "metadata": {},
   "source": [
    "**NOTE**: as most other PLiNIO methods, `MPS` supports passing a **cost model** to the constructor, to specify non-functional optimization metric(s), e.g., model size, latency, energy, etc. However, since this example refers to a single-precision QAT (so both the network architecture and the precision are fixed), PLiNIO has *no freedom to optimize cost*, and the cost model is irrelevant.\n",
    "\n",
    "In other words, after exporting the model and compiling it for edge inference, its size will be reduced by approximately 4x moving from float32 to int8, and latency will also most probably improve thanks to the usage of integer arithmetics. However, PLiNIO has no way to *alter the cost vs accuracy trade-off at training time* since the only available option is 8-bit quantization. \n",
    "\n",
    "Therefore, in this case, we can omit the cost model from the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57069995-37d2-4952-a391-bbf8526b5d65",
   "metadata": {},
   "source": [
    "### Evaluating the Converted Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f968c0a-5ad1-4508-814b-32f332e6d1db",
   "metadata": {},
   "source": [
    "The model generated by the MPS constructor has fake-quantization operations to simulate `int8` precision. Furthermore, other optimizations are performed during the conversion, such as folding Batch Normalization layers with Convolutions or Linear layers. Overall, the result of the conversion is similar to what we would get with a (very basic) Post-Training Quantization (PTQ). Let's check how this model performs on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674479d-a358-4a30-993a-4f44fe6488ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "test_metrics = evaluate(mps_model, criterion, test_loader, device)\n",
    "print(f'Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1532046-d167-49d7-b598-b47b2efc9c3d",
   "metadata": {},
   "source": [
    "As you can see, the performance drops significantly. Note that this is due to PLiNIO's MPS initialization currently not being particularly smart (e.g., all activation quantizers' initial ranges are set to the same default value, rather than being tuned to each tensor's distribution). However, QAT can usually recover this drop in a few epochs. Let's try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PI6CEP-PJdnO",
   "metadata": {
    "id": "PI6CEP-PJdnO"
   },
   "source": [
    "## 5) Running the Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qb8qLFW8Bti7",
   "metadata": {
    "id": "Qb8qLFW8Bti7"
   },
   "source": [
    "We are now ready to run QAT loop. Note that, if we wanted to actually *select* the bitwidth using MPS, we would have to implement something more similar to the `prune()` function in [this](https://github.com/eml-eda/plinio/blob/main/examples/channel_pruning_pit.ipynb) notebook (which exemplifies the PIT channel-pruning method). However, we're keeping a fixed precision, as discussed above. So, in this case, we can *entirely reuse* the `train()` function defined above, just passing the `mps_model` to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e5518-e610-4e4d-aaeb-2169eb0a462b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = SAVE_DIR / \"qat_best.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mps_model.parameters(), lr=TRAINING_CONFIG['qat_lr'], weight_decay=TRAINING_CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_CONFIG['qat_epochs'])\n",
    "\n",
    "history = train(checkpoint_path, TRAINING_CONFIG['qat_patience'], TRAINING_CONFIG['qat_epochs'], mps_model, criterion, optimizer, scheduler,\n",
    "      train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead6beb-08f8-45aa-9188-60c02744a75f",
   "metadata": {},
   "source": [
    "The QAT should terminate after few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7b18a-7fa9-42c8-9513-758943e397ea",
   "metadata": {
    "id": "2ee795d5-cadb-45fe-92db-1cceb1638ffb"
   },
   "source": [
    "### Evaluating the Fake-quantized Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6c8bc-85a5-42fb-8099-d916ef2b7274",
   "metadata": {},
   "source": [
    "Let's check the test accuracy of the fake-quantized DNN after QAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625e43f-b8d0-462c-85a0-af8871fc04b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4089ce-bfe8-4768-8456-e44d6c37b684",
    "outputId": "279110db-8205-4391-b1c6-c14bd11df777",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mps_model.load_state_dict(torch.load(checkpoint_path))\n",
    "mps_model.eval()\n",
    "\n",
    "test_metrics = evaluate(mps_model, criterion, test_loader, device)\n",
    "print(f'Exported Model Test Loss: {test_metrics[\"loss\"]}, Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3344c2c-083c-4470-b594-1d69d5715fce",
   "metadata": {},
   "source": [
    "The model should have now recovered the same validation accuracy of the float version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aRJOELwKSP2",
   "metadata": {
    "id": "6aRJOELwKSP2"
   },
   "source": [
    "## 6) Final Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7lXI9Y6lKfhF",
   "metadata": {
    "id": "7lXI9Y6lKfhF"
   },
   "source": [
    "We can now call the `.export()` method of the PLiNIO MPS model. For other PLiNIO methods (SuperNet, PIT, etc) `.export()` returns a vanilla Torch model. However, in the case of MPS, we need to preserve quantization information, and the corresponding parameters. Therefore, exporting the model causes each quantized layer to be replaced with an instance of a custom quantized class (for instance, `nn.Conv2D` becomes `QuantConv2D`). These layers function analogously to the torch equivalents, but also store the quantization parameters (e.g. min/max values for each weight tensor), and use them to simulate the effect of quantization during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IfBE26P9EvzI",
   "metadata": {
    "id": "IfBE26P9EvzI"
   },
   "outputs": [],
   "source": [
    "quant_model = mps_model.export()\n",
    "quant_model = quant_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Ahd0um3Kxkt",
   "metadata": {
    "id": "3Ahd0um3Kxkt"
   },
   "source": [
    "Let's look at the exported model using `torchinfo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MhDpsvm8K0b0",
   "metadata": {
    "id": "MhDpsvm8K0b0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(summary(quant_model, (1,) + input_shape, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b7957-8705-429b-8746-c6f5620b782b",
   "metadata": {},
   "source": [
    "The printout shows the `Quant*` modules with their internal quantizers (`MinMaxWeight`,`QuantizerBias` and  `PACTAct` for weights, biases and activations respectively). Note that some layers do not have all quantizers as internal modules as PLiNIO automatically identifies *sharing constraints* (e.g., layers that must have the same output quantizer because their respective outputs are summed together)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff20dd8-44c4-4f0b-a32b-ebedbb0a7cc0",
   "metadata": {},
   "source": [
    "### Export to ONNX for AI Compilation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933ffdc4-7c3e-4987-a95f-0a85f2993808",
   "metadata": {},
   "source": [
    "PLiNIO also supports exporting a quantized model to [ONNX](https://onnx.ai/) format, which is often used by AI compilers, especially for embedded/edge devices, for generating low-level code for the target. While the generated file follows the ONNX standard, tested compatibility with AI compilers is currently limited to [MATCH](https://arxiv.org/abs/2410.08855) (suggested) and [DORY](https://arxiv.org/abs/2008.07127) (deprecated). To generate a full-integer ONNX, we need two final steps:\n",
    "\n",
    "- Integerization, which converts the model from fake-quantized to true-integer (i.e., all parameters, including scale factors for quantizers, are rounded to integers at the target precision.\n",
    "- Backend exporting.\n",
    "The following cell integerizes the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832a7d1-e838-4652-ba86-08551fd834fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the model to full-integer, compiler-compliant format\n",
    "full_int_model = integerize_arch(quant_model.cpu(), Backend.ONNX, backend_kwargs={'shift_pos': 24})\n",
    "#full_int_model = full_int_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38395620-20ab-49c0-a373-41b09bb7d1af",
   "metadata": {},
   "source": [
    "Evaluate the full integer model to verify that accuracy has been preserved (differences, if any, should be minimal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce546c9-588e-4469-a7c9-b8547307a02d",
   "metadata": {
    "id": "DVFyXZ7nLTky",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_metrics = evaluate(full_int_model, criterion, test_loader, device)\n",
    "test_metrics = evaluate(full_int_model, criterion, test_loader, 'cpu')\n",
    "print(f'Full Integer Model Test Acc: {test_metrics[\"acc\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f032825-71f6-446a-941a-5de14a32856d",
   "metadata": {},
   "source": [
    "Lastly, let's generate the output ONNX and save it to the DATA_DIR folder. You can open the `.onnx` file with a tool like [Netron](https://netron.app/) to verify its correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KyC4xItHLTk0",
   "metadata": {
    "id": "KyC4xItHLTk0"
   },
   "outputs": [],
   "source": [
    "exporter = ONNXExporter()\n",
    "exporter.export(full_int_model, (1,) + input_shape, DATA_DIR)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
